{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This is a prototype that was created to test the concepts that would later be adapted to the python files that come under main.py. By confirming the concepts worked on a smaller data set it became easier to migrate the code from this jupyter notebook into main.py, ingest.py, preprocess.py, analyse.py and visualise.py.",
   "id": "ab39fb59eb577adc"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-03T23:53:16.171894Z",
     "start_time": "2025-11-03T23:53:16.159335Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "file_path = Path(\"../data/sample_stream.jsonl\")\n",
    "data=[]\n",
    "\n",
    "if not file_path.exists():\n",
    "    print(f\"File not found! Looked in: {file_path.resolve()}\")\n",
    "else:\n",
    "    print(f\"Reading from: {file_path.resolve()}\\n\")\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f, 1):\n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "                data.append(obj)\n",
    "                print(f\"Line {i} {json.dumps(obj, indent=2)}\")\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Line {i}: (invalid JSON) {line.strip()}\")\n",
    "\n",
    "print (\"\\nFinished reading file.\")\n",
    "print(f\"Total valid JSON objects: {len(data)}\")"
   ],
   "id": "4939adb7934a1b59",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Cell One: Reads the .jsonl file line by line, parsing each entry into a JSON object and storing it into a list. This allows the structured format to be preserved, simplifying the data processing for in later cells. Error handling was also added to help with debugging path issues.",
   "id": "876cf0d593d25870"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T23:53:16.998886Z",
     "start_time": "2025-11-03T23:53:16.305209Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "punct_to_remove = ''.join([p for p in string.punctuation if p not in [\"!\", \"?\", \"'\"]])\n",
    "\n",
    "def preprocess_text_vader(text):\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', punct_to_remove))\n",
    "    tokens = text.split()\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "    return ' '. join(tokens)\n",
    "\n",
    "df ['processed_text'] = df['text'].apply(preprocess_text_vader)\n",
    "processed_data = df[['timestamp', 'processed_text']]\n",
    "\n",
    "print(processed_data)"
   ],
   "id": "adc1e7d6a5ce2f93",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Cell Two: Cleans and preprocesses the text data for sentiment analysis in Cell Three. Each entry is converted to lowercase, and punctuation is removed except for \"!\", \"?\", and \"'\" to maintain emotional indicators and preserve word meaning. Stopwords are also filtered out. pandas is used for an efficient way of processing the entire column instead of iterating through all the lines (this was especially necessary in Cell Two as many operations are required as opposed to cell one). Using string helps simplify the management of punctuation, and nltk provides the stopwords. By preprocessing the data, this helps standardize the text, which should improve the consistency of VADER. The results of the preprocessing are stored in a new column, leaving the original data unchanged for future reference. This data is then saved to processed_data.",
   "id": "d935cb90d410b86"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T23:53:17.089649Z",
     "start_time": "2025-11-03T23:53:17.032075Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "import pandas as pd\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "processed_data['vader_scores'] = processed_data['processed_text'].apply(lambda x: sia.polarity_scores(x))\n",
    "processed_data[['neg', 'neu', 'pos', 'compound']] = processed_data['vader_scores'].apply(pd.Series)\n",
    "\n",
    "def label_sentiment(compound):\n",
    "    if compound >= 0.05:\n",
    "        return 'Positive'\n",
    "    elif compound <= -0.05:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "processed_data['sentiment'] = processed_data['compound'].apply(label_sentiment)\n",
    "display_columns = ['timestamp', 'processed_text', 'neg', 'neu', 'pos', 'compound', 'sentiment']\n",
    "vader_processed_data = processed_data[display_columns]\n",
    "\n",
    "print (vader_processed_data)"
   ],
   "id": "5e3990fe15dc582e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Cell Three: Applies sentiment analysis to the processed_data using VADER. Each column is scored to produce a score indicating negative, neutral and positive sentiment. Without documentation knowledge the scores are hard to understand, the function label_sentiment helps create a threshold used to label each entry. The data now labelled by Vader is then saved to vader_processed_data.",
   "id": "7e366b23511d8cbd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T23:53:18.960200Z",
     "start_time": "2025-11-03T23:53:17.124407Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "plot_data = vader_processed_data.copy()\n",
    "plot_data ['timestamp'] = pd.to_datetime(plot_data['timestamp'])\n",
    "\n",
    "output_dir = \"../output/prototype_outputs\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "#Line Plot showing sentiment changes over time\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(plot_data['timestamp'], plot_data['compound'], marker= 'o', linewidth=2, markersize=6, color='steelblue', alpha=0.8)\n",
    "plt.axhline(y=0, color= 'r', linestyle='--', alpha=0.3, label='Neutral Line')\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%H:%M'))\n",
    "plt.gca().xaxis.set_major_locator(mdates.MinuteLocator(interval=5))\n",
    "plt.title('Sentiment Compound Score Over Time')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Compound Sentiment Score')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'sentiment_overtime.png'))\n",
    "plt.show()\n",
    "\n",
    "#Count Plot showing sentiment distribution\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(x='sentiment', data=vader_processed_data, hue='sentiment', palette= 'pastel', order=['Positive', 'Neutral', 'Negative'], legend=False)\n",
    "plt.title(\"Sentiment Distribution (Count Plot)\")\n",
    "plt.xlabel(\"Sentiment\")\n",
    "plt.ylabel(\"Number of Reviews\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'sentiment_distribution_count.png'))\n",
    "plt.show()\n",
    "\n",
    "#Average Sentiment scores by label\n",
    "avg_scores = vader_processed_data.groupby('sentiment')[['neg', 'neu', 'pos']].mean().reset_index()\n",
    "avg_scores_melted = avg_scores.melt(id_vars= 'sentiment', value_vars=['neg', 'neu', 'pos'], var_name='Score Type', value_name='Average')\n",
    "\n",
    "#Bar plot showing the average sentiment scores by label\n",
    "plt.figure(figsize=(7,4))\n",
    "sns.barplot(x='sentiment', y='Average', hue='Score Type', data=avg_scores_melted, palette= 'muted')\n",
    "plt.title(\"Average Sentiment Scores by label\")\n",
    "plt.ylabel(\"Average Score\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'average_sentiment_scores.png'))\n",
    "plt.show()\n",
    "\n",
    "#Pie chart showing how sentiment is distributed\n",
    "plt.figure(figsize=(6,6))\n",
    "sentiment_counts = vader_processed_data['sentiment'].value_counts().reindex(['Positive', 'Neutral', 'Negative'])\n",
    "sentiment_counts = sentiment_counts.fillna(0)\n",
    "labels = sentiment_counts.index.tolist()\n",
    "sizes = sentiment_counts.values\n",
    "plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=140, wedgeprops={'edgecolor':'w'} )\n",
    "plt.title(\"Sentiment Distribution (Pie Chart)\")\n",
    "\n",
    "legend_labels = [f\"{lbl}: {cnt}\" for lbl, cnt in zip(labels, sizes)]\n",
    "plt.axis('equal')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'sentiment_distribution_pie.png'))\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nAll plots saved to {os.path.abspath(output_dir)}\")"
   ],
   "id": "7a7932db9d8e6f06",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Cell Four: Visualisations are now implemented to help understand patterns and distributions within vader_processed_data. A variety of visualisations were chose to make the data more explainable. Line plot: Shows the change in compound sentiment allowing for indications of trends or spikes. Count plot: Displays the frequency for each sentiment label displaying which sentiments are dominant. Bar plot: compares the average negative, neutral and positive scores across the sentiment labels to show relative strength. Pie chart: Illustrates the overall proportion of each sentiment category. matplotlib gave the most flexible plotting framework for creating and saving these plots. seaborn was used to improve the aesthetics of the plots for easier understanding. The plots are then saved within the output folder within a folder called prototype_outputs as .pngs labelled according to their diagram.\n",
   "id": "5aba35134eb82990"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T23:53:20.100920Z",
     "start_time": "2025-11-03T23:53:20.085614Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "output_path = \"../output/prototype_outputs/sentiment_analysis.csv\"\n",
    "\n",
    "output_dir = os.path.dirname(output_path)\n",
    "if output_dir and not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "to_save = vader_processed_data.copy()\n",
    "to_save.to_csv(output_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"\\n File successfully saved to: {os.path.abspath(output_path)}\")\n",
    "print(f\"Total rows saved: {len(to_save)}\")"
   ],
   "id": "1c973d82eecae842",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Cell Five: Saves the processed sentiment data into a CSV file in the prototypes_outputs folder alongside the visualisations. pandas helps efficiently export the DataFrame whilst preserving the columns including the original text, preprocessed text, sentiment scores and sentiment labels. os ensures that the output directory is correct before saving to prevent any potential errors. This cell guarantees that the analysis results are stored persistently for future reference.",
   "id": "a0362eec4f385685"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}